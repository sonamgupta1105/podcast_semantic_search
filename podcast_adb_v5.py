# -*- coding: utf-8 -*-
"""podcast_adb_v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18s792Ih1IpMWCpVHKHw96cWrlG5UekbX
"""

!pip install --upgrade --quiet aperturedb
!pip install tiktoken
!pip install aperturedb
!pip install cohere
!pip install langchain
!pip install langchain-community
!pip install langchain --upgrade
!pip install --quiet langchain_experimental
!pip install -U langchain-cohere

import warnings
warnings.filterwarnings('ignore')
import logging

# Disable all logging messages
logging.disable(logging.CRITICAL)

# Dependencies
import json
from aperturedb.Images import Images
from aperturedb.Utils import Utils, create_connector
from aperturedb import NotebookHelpers as nh
from aperturedb.Constraints import Constraints
from aperturedb import BlobDataCSV
import os
import cohere
import numpy as np
from aperturedb import ParallelLoader
from aperturedb import Descriptors
from langchain_community.vectorstores import ApertureDB
from langchain.chains import RetrievalQA
from langchain_cohere import CohereEmbeddings
from langchain_text_splitters import TokenTextSplitter
from langchain_cohere import ChatCohere
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from aperturedb.CommonLibrary import create_connector

!adb config ls

# import logging
# for h in logging.getLogger('aperturedb').handlers:
#   h.setLevel(logging.ERROR)

!adb config create podcast_search --host admin-vdgd8ys9.farm0000.cloud.aperturedata.io --password T6^zXh*EfGHQl0Fm --no-interactive --active

!adb utils execute status

# %env LOG_CONSOLE_LEVEL=ERROR
# Connect to the ApertureDB instance.
client = create_connector()
utils = Utils(client)

# Empty the database
#utils.remove_all_objects()

# import logging
# logging.basicConfig(level=logging.WARNING, force=False)

from pickle import FALSE
text_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=100)
os.environ["COHERE_API_KEY"] = "ALIqNxakYXjum0chFSpfAzbw7XnpsWYS1Hn2ka9d"
cohere_embeddings = CohereEmbeddings(model="embed-english-v3.0")

DESCRIPTOR_SET = "cohere_embed-english-v3.0"
aperturedb_vectorstore = ApertureDB(descriptor_set=DESCRIPTOR_SET, embeddings=cohere_embeddings)

# This will eventually become GraphStore commands.

q = [
  {
    "FindBlob": {
      "constraints": {
        "type": [
          "==",
          "transcript"
        ]
      },
      "blobs": True,
      "results": {
        "all_properties": True
      }
    }
  }
]

responses, blobs = aperturedb_vectorstore.connection.query(q)
documents = []
for e in responses[0]["FindBlob"]["entities"]:
  b = blobs[e["_blob_index"]]
  text = b.decode("utf-8")
  split_texts = text_splitter.split_text(text)
  for i, t in enumerate(split_texts):
    metadata = { k: e[k] for k in ["guest_name", "title", "profession", "guest_linkedin", "guest_company", "page_url"]}
    metadata["url"] = e["page_url"]
    metadata["podcast_title"] = e["title"]
    metadata["guest_profession"] = e["profession"]
    metadata["guest_profile"] = e["guest_linkedin"]
    metadata["company"] = e["guest_company"]
    metadata["segment"] = i
    document = Document(t, metadata=metadata)
    documents.append(document)
print(len(documents))

ids = aperturedb_vectorstore.add_documents(documents)
print(len(ids))

"""# Semantic Search"""

# Query
query = "What are the guests in this podcast think about LLMs?"
docs = aperturedb_vectorstore.similarity_search(query, k = 5)
print(docs)

from pprint import pprint
for d in docs:
  pprint(d.page_content)

"""# Chat Module"""

search_type = "mmr" # "similarity" or "mmr"
k = 3              # number of results used by LLM
fetch_k = 25       # number of results fetched for MMR
retriever = aperturedb_vectorstore.as_retriever(search_type=search_type,
    search_kwargs=dict(k=k, fetch_k=fetch_k))

from langchain_cohere import ChatCohere

llm = ChatCohere(model="command-r")

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate

template = """

  You are a question & answering chat assistant. Use the podcast transcripts provided below to answer the questions at the end. Also share the metadata associated with the answer you provide in bullet point format.

  Also if the question is covered in multiple podcasts, use the context from all of it.

  Give as detailed answer as possible.

  If the answer to the question is not contained in the provided podcast transcripts, say "The answer is not in the context".
  query: {query}

  {context}
"""
# using chain
prompt = ChatPromptTemplate.from_template(template)
model = ChatCohere(model="command-r")
chain = (
    {
        "context" : retriever,
        "query" : RunnablePassthrough()
    }
    | prompt | model | StrOutputParser()

)

from pprint import pprint

pprint(chain.invoke("""How does Retrieval Audmented Generation technique help in building LLM applications?"""))

print(chain.invoke("""Do you think real-time data streaming is essential these days?"""))

print(chain.invoke("""What is knowledge graph?"""))

print(chain.invoke("""What are some of the AI industry highlights you can share?"""))

print(chain.invoke(""" What is the purpose of agents?"""))

print(chain.invoke("""How do you think AI is helping people to be more productive?"""))

print(chain.invoke("""Do you think AI will steal our jobs?"""))